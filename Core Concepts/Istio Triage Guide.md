# Istio Triage Guide

## Functional Components
| Component Name | Responsibilities | Platform | Sample Configuration |
| ------ | ------ | ------ | ------ |
| IstioD | IstioD provides service discovery, configuration and certificate management| Control plane/ Data plane | NA
Istio Ingress Gateway | A loadbalancer (Standalone Envoy Proxy) operating at the edge of the mesh that receives incoming HTTPS/TCP traffics | Control Plane | NA
Gateway | Gateway sits at the edge of the mesh and receives all the incoming connections on the behalf of the PODs. The Istio Ingress Gateway also provide option of _SSL Termination_ at the Gateway level.Terminating SSL connection at the Gateway level helps in offloading the computing task to decrypt traffic from the application leve, which improves apllication performance. PostSSL termination at the Gateway, traffic will be encrypted and decrypted back again using Istio mTLS capabilities. For Service to Service authentication as well, Istio mTLS ensures traffic is encrypted on transit between services and CITADEL in control plane, and it is responsible for issuing and managing certificates. <br/><br/>**Note** : Make sure to use only one Gateway for one namespace to avoid the Internal Server Errors | Control Plane | [GW]()
Virtual Service and Destination Rule | While the VirualService defines configuration affecting the traffic routing, a DestinationRule defines policies that apply to traffic intended for a service after routing has occured. The resources are tightly linked, and first DestinationRules will be applied before updating VirtualService so as to avoid unintentional traffic flow. For instance, when introducing a newer version of service, define the DestinationRule subset with rules for whatwill reach that service before introducing the VirtualService to route traffic to it. | Control Plane | [VS]()
Service  | As Kubernetes resolves DNS on a cluster basis and because the DNS resolution is tied to the cluster, service object in every cluster will be defined where a client runs, regardless of the location of the service's endpoints. For ensuring this, duplication of the service object of to every cluster using _kubectl_  will be performed. Duplication ensures Kubernetes can resolve the service name in any cluster. Since the service objects are defined in a namespace and needs to be included in the service definations in all clusters. <br/><br/>**Note** : The name of the service should match with the name of _http.destination.host_ of the VirtualService | Workload Cluster | [SERVICE]()
Service Entry | All outbound traffic from an Istio-enabled Pod is redirected to its Sidecar Proxy by default, accessibility of URLs outside of the cluster depends on the configuration of the proxy. By default Istio configures the Envoy proxy to pass through the requests for unknown services, so for a controlled way of enabling access to external services, the meshConfig.outboundTrafficPolicy.mode need to be configured to the REGISTRY_ONLY mode. To access external services, ServiceEntry would be configured to provide controlled access to external services. This will help in stopping egress leaking. <br/><br/>**Note** : For Pods accessing external services which are out of the ServiceMesh, the ServiceEntry has to be created and deployed.<br/><br/>**Troubleshooting**<br/>To check whether a Pod is able to connect to downstream service <br/><br/>1. Make a request to the external HTTPS service from the _SOURCE POD_ and see the response, if the response is _200 OK_ then downstream connectivity is working fine <br/>`kubectl exec -it pod/<source pod name> -c sleep -- curl -I https://vault.com` <br/> `200 OK`<br/><br/>2. If the response is not _200_, and you are getting something like _Connection Reset by Peer_ or _400_, _401_, _500_ ERROR. Then the first step is to check the Istio Proxy Logs in the Istio Proxy Container<br/>`kubectl logs pod/<source pod> -c istio-proxy -f -n <namespace>` <br/><br/>If the _BLACKHOLE CLUSTER_ problem is there in the log that means the ServiceEntry is wrongly configured which is not allowing the Istio proxy to to route the egress traffic to the desired endpoint<br/><br/>Possible Reasons for This Error:<br/>1.Desired downstream endpoint CIDR is wrong<br/>2. Mismatch in the name and protocol<br/>3.CIDR used in two serviceentries are overlapping. In this situation always use _HOSTNAMES_ of the endpoints and remove CIDR, also change RESOLUTION to DNS | Control Plane | [SE]()

## Traffic Communication Pattern:

### 1. Traffic communications between PODS within same Namespace
Recommended practice for PODs communicating within same namespace is to communicate using service name of the pod i.e. `<podname>.svc.cluster.local` instead of using _pods ingress url_ to communicate locally

### 2. Namespace to Namespace Traffic communications within Istio Mesh
Traffic communication between namespaces with in the mesh happens via istio-proxy container (Envoy Proxy) of the pod. Sidecar describes the configuration of the sidecar proxy that mediates inbound and outbournd comimunication to the workload instance it is attached to. The Sidecar configuration will be used to fine tune the set of ports, protocols that the proxy will accept when forwarding traffic to and from the workload to flowing to and fro from all ports. In addition to this, when forwarding outbound traffic from workload instances, it will be restricted to a limited set of services that the proxy can reach. For communication between Pod Namespace but same ISTIO Mesh, configure `<podname>.<namesopacename>.svc.cluster.local`

### 3. Network Traffic communications between PODS within Mesh to GCP/AWS services outside of Mesh:

Traffic Communication between, external service outside of Mesh such as vault, dynatrace but in either another cluster or other cloud services will happen through Istio. Service Entry for Egress connection and Istio In Gateway for ingress connection'as depicted in diagram: PODS _East-West_ and _North-South Traffic_  <br/>Along with this for the traffic going outside of the cluster, it might require to create NACL rule to allow ingress and egress traffic.

### 4. Network Traffic communications between PODS within Mesh to on-prem services outside of Mesh:

Traffic Communication between external services such as PingFed, MongoDB and Oracle DB running on-prem and PODS within Mesh will happen through Istio Ingress and Service Entry as depicted in diagram: "PODS and North-South Traffic" below. Along with this for the traffic going outside of GKE cluster where MLaaS PODS are hosted, it would require to create a firewall rule on GKE eluster to allow ingress and egress traffic.

## Network Policies
Along with Istio level ingress and egress control, to control traffic flow at the IP address or port level (OSI layer 3 or 4), Kubernetes Network Policies are applied in Namespace Lavel. Network Policies are an application-centric construct which allow to specify how a pod is allowed to communicate with various network "entities" over the network Network Policies apply to a connection with a pod on one or both ends, and are not relevant to other connections.

Below are the conditions where network policies are generally used for Tenant PODS. POD traffic can traverse through a combination of the following 3. identifiers. Along with this wheh detining a pod- or namespace- based NetworkPolicy, at times selectors are also utilised to specify what traffic is allowed to and from the Pod(s) that match the selector.
- All PODS within a namespace are allowed to communicated by using a network policy "Allow all-Network policy" 
- PODS in separate namespaces but part of same istio mesh via Ingress and Egress Network policy on specific ports.
- PODS to external service traffic communication such as for API Mesh, Mongo DB via egress network policy with traffic restricting to destination CIDR and port
